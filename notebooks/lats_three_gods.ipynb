{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbfc12ba-a873-46ae-a3cc-a6c449db64a7",
   "metadata": {},
   "source": [
    "# LATS - Three Gods Puzzle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba582a",
   "metadata": {},
   "source": [
    "Before starting run `nix develop .` to set the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8992b248-cb8e-4ea9-9ed7-4e7dbf7efc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pprint import pp\n",
    "from pyswip import Prolog\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import uuid\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# env assertions\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY is not set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb6d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_begin_end_regex = re.compile(\n",
    "    r\"% -- remove_begin -- %.*?% -- remove_end -- %\", re.DOTALL\n",
    ")\n",
    "\n",
    "\n",
    "def read_facts_for_llm(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        file_content = f.read()\n",
    "        clean_content = remove_begin_end_regex.sub(\"\", file_content)\n",
    "        return clean_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc429a7-5761-4e48-9307-c0a1067505ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUZZLE = \"\"\"\n",
    "<puzzle>\n",
    "Three gods A, B, and C are called, in no particular order, True, False, and Random. True always speaks truly, False always speaks falsely, \n",
    "but whether Random speaks truly or falsely is a completely random matter. \n",
    "Your task is to determine the identities of A, B, and C by asking three yesâ€“no questions; each question must be put to exactly one god. \n",
    "The gods understand English, but will answer all questions in their own language, in which the words for yes and no are da and ja,[3] in some order. \n",
    "You do not know which word means which.\n",
    "</puzzle>\n",
    "\"\"\"\n",
    "\n",
    "PARTIAL_FACTS_FOR_LLM = (\n",
    "    \"<partial_prolog_facts>\\n\"\n",
    "    + read_facts_for_llm(\"three_gods_inference.pl\")\n",
    "    + \"\\n</partial_prolog_facts>\"\n",
    ")\n",
    "\n",
    "PROBLEM_PROMPT_V1 = f\"\"\"\n",
    "You are solving the Three Gods puzzle using the SWI-Prolog logic programming language.\n",
    "\n",
    "{PUZZLE}\n",
    "\n",
    "{PARTIAL_FACTS_FOR_LLM}\n",
    "\n",
    "Return a single question for one god using either ask/3 or meta_ask/3, and do not include explanations.\n",
    "\"\"\"\n",
    "\n",
    "EXE_PROMPT_V1 = f\"\"\"\n",
    "You are solving the Three Gods puzzle using the SWI-Prolog logic programming language.\n",
    "\n",
    "{PUZZLE}\n",
    "\n",
    "Think step by step and output the list of predicted god identities.\n",
    "\n",
    "The god schema is as follows:\n",
    "\n",
    "identifier: one of [a, b, c]\n",
    "value: one of [true_god, false_god, random_god]\n",
    "\"\"\"\n",
    "\n",
    "EVAL_PROMPT_V1 = f\"\"\"\n",
    "Given this logic puzzle and partial Prolog facts.\n",
    "\n",
    "{PUZZLE}\n",
    "\n",
    "{PARTIAL_FACTS_FOR_LLM}\n",
    "\n",
    "Score this predicate from 1 to 10 toward solving the puzzle. Output only an integer with no explanation.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "SELF_CONSISTENCY_PROMPT_V1 = \"\"\"\n",
    "Given intermediate predicates in Prolog logic programming language (SWI-Prolog) and their results, determine how self-consistent they are.\n",
    "\n",
    "Score from 1 to 10 (10 = most consistent, 1 = least). Output only an integer with no explanation.\n",
    "\"\"\"\n",
    "\n",
    "REFLECT_PROMPT_V1 = \"\"\"\n",
    "You are solving the Three Gods puzzle using the SWI-Prolog logic programming language.\n",
    "Your previous attempt failed to identify all three gods.\n",
    "\n",
    "{puzzle}\n",
    "\n",
    "{partial_prolog_facts}\n",
    "\n",
    "YOUR FAILED ATTEMPT:\n",
    "{latest_failed_trajectory}\n",
    "\n",
    "FINAL STATE:\n",
    "{final_state}\n",
    "\n",
    "Analysis required:\n",
    "1. What information did you gain from each predicate?\n",
    "2. What information is still missing?\n",
    "3. Which god(s) could you not identify and why?\n",
    "4. Did you waste any questions on redundant information?\n",
    "5. Did you handle the da/ja ambiguity correctly?\n",
    "6. Did you account for the Random god's unpredictability?\n",
    "\n",
    "Reflection:\n",
    "Based on this failure, what strategy would work better? Be specific about:\n",
    "- Which god to ask first and why\n",
    "- What type of question to use (direct vs counterfactual)\n",
    "- How to handle the language ambiguity\n",
    "- How to isolate the Random god\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GodSchema(BaseModel):\n",
    "    identifier: str\n",
    "    value: str\n",
    "\n",
    "\n",
    "class GodListSchema(BaseModel):\n",
    "    gods: list[GodSchema]\n",
    "\n",
    "\n",
    "# Load the Prolog helper predicates that back this notebook.\n",
    "Prolog.consult(\"three_gods_inference.pl\")\n",
    "query = Prolog.query(\n",
    "    \"meta_ask(b, (a = true_god), Answer).\", catcherrors=True, normalize=True\n",
    ")\n",
    "print([i for i in query][0])\n",
    "\n",
    "\n",
    "def god_schema_to_predicate(gods: list[GodSchema]) -> list[str]:\n",
    "    return [f\"god({god.identifier}, {god.value}).\" for god in gods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c1b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntSchema(BaseModel):\n",
    "    value: int\n",
    "\n",
    "\n",
    "class Node:\n",
    "    pass\n",
    "\n",
    "\n",
    "class Generator:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI()\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        context: str,\n",
    "        reflection: str,\n",
    "        state: str,\n",
    "        latest_failed_trajectory: list[Node],\n",
    "    ) -> str:\n",
    "        latest_failed_trajectory_str = \"\\n\".join(\n",
    "            [node.state for node in latest_failed_trajectory]\n",
    "        )\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Reflection from previous runs: {reflection}\n",
    "        Failed trajectory: {latest_failed_trajectory_str}\n",
    "        Steps so far: {context}\n",
    "        Current state: {state}\n",
    "        Next step is:\"\n",
    "        \"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": PROBLEM_PROMPT_V1},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def generator_impl():\n",
    "    return Generator()\n",
    "\n",
    "\n",
    "class Executor:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI()\n",
    "\n",
    "    def execute(self, context: str, state: str) -> tuple[int, str]:\n",
    "        prompt = f\"Questions asked so far: {context}\\nCurrent question: {state}\"\n",
    "        response = self.client.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": EXE_PROMPT_V1},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            response_format=GodListSchema,\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.parsed.gods\n",
    "\n",
    "\n",
    "def executor_impl():\n",
    "    return Executor()\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI()\n",
    "\n",
    "    def evaluate(self, context: str, state: str) -> int:\n",
    "        prompt = (\n",
    "            f\"Questions asked so far: {context}\\nCurrent question: {state}\\n Score is:\"\n",
    "        )\n",
    "        response = self.client.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": EVAL_PROMPT_V1},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            response_format=IntSchema,\n",
    "        )\n",
    "\n",
    "        return round(response.choices[0].message.parsed.value / 10, 2)\n",
    "\n",
    "    def self_consistency(\n",
    "        self,\n",
    "        context: str,\n",
    "        reflection: str,\n",
    "        state: str,\n",
    "        latest_failed_trajectory: list[Node],\n",
    "    ) -> int:\n",
    "        sc_sampling_size = 3\n",
    "        prompt = \"\\n---\\n\".join(\n",
    "            [\n",
    "                generator_impl().generate(\n",
    "                    context, reflection, state, latest_failed_trajectory\n",
    "                )\n",
    "                for _ in range(sc_sampling_size)\n",
    "            ]\n",
    "        )\n",
    "        response = self.client.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SELF_CONSISTENCY_PROMPT_V1},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            response_format=IntSchema,\n",
    "        )\n",
    "        return round(response.choices[0].message.parsed.value / 10, 2)\n",
    "\n",
    "    def reflect(self, context: str, state: str) -> str:\n",
    "        reflection_prompt = REFLECT_PROMPT_V1.format(\n",
    "            puzzle=PUZZLE,\n",
    "            partial_prolog_facts=PARTIAL_FACTS_FOR_LLM,\n",
    "            latest_failed_trajectory=context,\n",
    "            final_state=state,\n",
    "        )\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": reflection_prompt},\n",
    "                {\"role\": \"user\", \"content\": \"Your reflection is?\"},\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def evaluator_impl():\n",
    "    return Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b196f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator_impl()\n",
    "executor = executor_impl()\n",
    "evaluator = evaluator_impl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_generated_actions = 3\n",
    "depth_limit = (\n",
    "    # Gods puzzle limits us to only consider 3 questions that we can ask gods.\n",
    "    3\n",
    ")\n",
    "number_of_rollouts = number_of_generated_actions * 2  # so reflection takes place\n",
    "exploration_weight = np.sqrt(2)\n",
    "value_weight = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e433a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(\n",
    "        self, state: str, context=\"\", parent=None, depth=0, self_reflection=\"\"\n",
    "    ):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.context = context\n",
    "        self.depth = depth\n",
    "        self.children = []\n",
    "        self.value = 0.0\n",
    "        self.visit_count = 0\n",
    "        self.self_reflection = self_reflection\n",
    "        self.reward = 0\n",
    "        self.is_terminal = False\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"id\": self.id,  # for debugging\n",
    "            \"state\": self.state,\n",
    "            \"parent\": self.parent.state if self.parent else None,\n",
    "            \"context\": self.context,\n",
    "            \"depth\": self.depth,\n",
    "            \"value\": self.value,\n",
    "            \"visit_count\": self.visit_count,\n",
    "            \"self_reflection\": self.self_reflection,\n",
    "            \"reward\": self.reward,\n",
    "            \"is_terminal\": self.is_terminal,\n",
    "            \"children\": [child.to_dict() for child in self.children],\n",
    "        }\n",
    "\n",
    "    def best_child_utc(self):\n",
    "        if not self.children:\n",
    "            return None\n",
    "        else:\n",
    "            return max(self.children, key=lambda child: child.uct())\n",
    "\n",
    "    def uct(self):\n",
    "        if self.visit_count == 0:\n",
    "            # open question: maybe it shouldn't be inf, in this case the non explored nodes will be preferred?\n",
    "            return float(\"inf\")\n",
    "        else:\n",
    "            return (self.value) + exploration_weight * np.sqrt(\n",
    "                np.log(self.parent.visit_count) / self.visit_count\n",
    "            )\n",
    "\n",
    "    def update(self, value):\n",
    "        self.visit_count += 1\n",
    "        self.value += round(value, 2)\n",
    "\n",
    "    def parent_states_as_string(self):\n",
    "        node = self\n",
    "        states = []\n",
    "\n",
    "        while node.parent is not None:\n",
    "            if \"No predicates asked yet\" not in node.state:\n",
    "                states.append(node.state)\n",
    "            node = node.parent\n",
    "\n",
    "        return \"\\n\".join(reversed(states))\n",
    "\n",
    "\n",
    "def generate_with_retry(\n",
    "    generator,\n",
    "    context,\n",
    "    reflection,\n",
    "    state,\n",
    "    latest_failed_trajectory,\n",
    "    retries=5,\n",
    "):\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            new_state = generator.generate(\n",
    "                context=context,\n",
    "                reflection=reflection,\n",
    "                state=state,\n",
    "                latest_failed_trajectory=latest_failed_trajectory,\n",
    "            )\n",
    "\n",
    "            prolog_eval_result = [\n",
    "                result\n",
    "                for result in Prolog.query(new_state, catcherrors=True, normalize=True)\n",
    "            ][0]\n",
    "\n",
    "            assert prolog_eval_result, \"Prolog result is empty\"\n",
    "\n",
    "            return new_state, json.dumps(prolog_eval_result)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating: {e}\")\n",
    "            continue\n",
    "\n",
    "    raise Exception(f\"Failed to generate prolog query after {retries} retries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2418459b-dbac-4d3a-b010-9c0ff947d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"******** Starting ********\")\n",
    "root = Node(state=\"No predicates asked yet\")\n",
    "root.visit_count = 1\n",
    "latest_failed_trajectory = []\n",
    "\n",
    "for _ in range(number_of_rollouts):\n",
    "    node = root\n",
    "    trajectory = []\n",
    "\n",
    "    for _ in range(depth_limit):\n",
    "        if len(node.children) == 0:\n",
    "            for _ in range(number_of_generated_actions):\n",
    "                new_state, prolog_eval_result = generate_with_retry(\n",
    "                    generator,\n",
    "                    context=node.context,\n",
    "                    reflection=node.self_reflection,\n",
    "                    state=node.state,\n",
    "                    latest_failed_trajectory=latest_failed_trajectory,\n",
    "                )\n",
    "\n",
    "                context = node.parent_states_as_string()\n",
    "\n",
    "                child = Node(\n",
    "                    state=new_state + \" => \" + prolog_eval_result,\n",
    "                    parent=node,\n",
    "                    context=context,\n",
    "                    depth=node.depth + 1,\n",
    "                    self_reflection=node.self_reflection,\n",
    "                )\n",
    "\n",
    "                lm_score = evaluator.evaluate(context=child.context, state=child.state)\n",
    "                sc_score = evaluator.self_consistency(\n",
    "                    context=child.context,\n",
    "                    reflection=child.self_reflection,\n",
    "                    state=child.state,\n",
    "                    latest_failed_trajectory=latest_failed_trajectory,\n",
    "                )\n",
    "                value = round(\n",
    "                    value_weight * lm_score + (1 - value_weight) * sc_score, 2\n",
    "                )\n",
    "\n",
    "                child.value = value\n",
    "\n",
    "                node.children.append(child)\n",
    "                print(\"******** child ********\")\n",
    "                pp(child.to_dict())\n",
    "\n",
    "        # select best child\n",
    "        if len(node.children) > 0:\n",
    "            node = node.best_child_utc()\n",
    "            node.visit_count += 1\n",
    "            trajectory.append(node)\n",
    "            print(\"******** Selected child ********\")\n",
    "            pp(node.to_dict())\n",
    "        else:\n",
    "            if not node.is_terminal:\n",
    "                trajectory.append(node)\n",
    "            break\n",
    "\n",
    "    # execute\n",
    "    gods = executor.execute(context=node.context, state=node.state)\n",
    "    god_predicates = god_schema_to_predicate(gods)\n",
    "\n",
    "    is_solved = all(\n",
    "        [\n",
    "            len([result for result in Prolog.query(predicate)]) > 0\n",
    "            for predicate in god_predicates\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if is_solved:\n",
    "        node.reward = 1\n",
    "    else:\n",
    "        latest_failed_trajectory = trajectory\n",
    "        node.self_reflection = evaluator.reflect(context=node.context, state=node.state)\n",
    "\n",
    "    reflection = node.self_reflection\n",
    "    print(\"******** reflection ********\")\n",
    "    print(reflection)\n",
    "\n",
    "    # backpropagate\n",
    "    for rev_node in reversed(trajectory):\n",
    "        print(\"******** Backpropagating ********\")\n",
    "        old_V = rev_node.value\n",
    "        old_N = rev_node.visit_count\n",
    "        value = (old_V * (old_N - 1) + is_solved) / old_N\n",
    "        rev_node.value = value\n",
    "\n",
    "        # `bubble-up` reflection\n",
    "        rev_node.self_reflection = reflection\n",
    "        print(\"reflection assigned to \", rev_node.id, rev_node.state)\n",
    "\n",
    "        if rev_node.parent is not None:\n",
    "            rev_node.parent.self_reflection = reflection\n",
    "            print(\"reflection assigned to \", rev_node.parent.id, rev_node.parent.state)\n",
    "\n",
    "    if is_solved == 1:\n",
    "        print(\"******** Final solution found ********\")\n",
    "\n",
    "        for node in reversed(trajectory):\n",
    "            print(\"******** Node ********\")\n",
    "            pp(node.to_dict())\n",
    "\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
